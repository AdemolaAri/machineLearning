{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "widgets": {
      "state": {},
      "version": "1.1.2"
    },
    "colab": {
      "name": "Logistic-Reg-WeeklyIslr.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdemolaAri/machineLearning/blob/master/Logistic_Reg_WeeklyIslr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "l3WfD38h4Mez",
        "colab_type": "text"
      },
      "source": [
        "<h1 align=\"center\"><font size=\"5\"> Logistic Regression: Python</font></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "egR6w1_b4Me3",
        "colab_type": "text"
      },
      "source": [
        "<a id=\"logit\"></a>\n",
        "## Introduction\n",
        "\n",
        "\n",
        "Logistic Regression is a variation of Linear Regression, useful when the observed dependent variable, <b>y</b>, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.\n",
        "\n",
        "Logistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate into a probability with the following function, which is called sigmoid function ùúé:\n",
        "\n",
        "$$\n",
        "‚Ñé_\\theta(ùë•) = \\sigma({\\theta^TX}) =  \\frac {e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +...)}}{1 + e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +\\cdots)}}\n",
        "$$\n",
        "Or:\n",
        "$$\n",
        "ProbabilityOfaClass_1 =  P(Y=1|X) = \\sigma({\\theta^TX}) = \\frac{e^{\\theta^TX}}{1+e^{\\theta^TX}} \n",
        "$$\n",
        "\n",
        "In this equation, ${\\theta^TX}$ is the regression result (the sum of the variables weighted by the coefficients), `exp` is the exponential function and $\\sigma(\\theta^TX)$ is the sigmoid or [logistic function](http://en.wikipedia.org/wiki/Logistic_function), also called logistic curve. It is a common \"S\" shape (sigmoid curve).\n",
        "\n",
        "So, briefly, Logistic Regression passes the input through the logistic/sigmoid but then treats the result as a probability:\n",
        "\n",
        "<img\n",
        "src=\"https://ibm.box.com/shared/static/kgv9alcghmjcv97op4d6onkyxevk23b1.png\" width=\"400\" align=\"center\">\n",
        "\n",
        "\n",
        "The objective of __Logistic Regression__ algorithm, is to find the best parameters Œ∏, for $‚Ñé_\\theta(ùë•)$ = $\\sigma({\\theta^TX})$, in such a way that the model best predicts the class of each case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q212dc-4Me4",
        "colab_type": "text"
      },
      "source": [
        "### Weekly Stock direction with Logistic Regression\n",
        "The Weekly Dataset available publicly through the ISLR library in R Studio contains 1,089 weekly stock returns for 21 years from the beginning of 1990 to the end of 2010.\n",
        "The dataset has 8 variables: \n",
        "\n",
        "  **Year**, 5 **Lag variables**, **Today**(Price) and the binary variable **Direction** (Down/Up)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "KjkwePlT4MfE",
        "colab_type": "text"
      },
      "source": [
        "###  Download the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "7mezGSo54MfF",
        "colab_type": "code",
        "outputId": "8e608182-e5a3-4207-d4dd-889d963b72da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!wget -O WeeklyStock.txt https://raw.githubusercontent.com/AdemolaAri/machineLearning/datasets/WeeklyStock.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-06 23:34:02--  https://raw.githubusercontent.com/AdemolaAri/machineLearning/datasets/WeeklyStock.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62683 (61K) [text/plain]\n",
            "Saving to: ‚ÄòWeeklyStock.txt‚Äô\n",
            "\n",
            "\rWeeklyStock.txt       0%[                    ]       0  --.-KB/s               \rWeeklyStock.txt     100%[===================>]  61.21K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-09-06 23:34:02 (4.12 MB/s) - ‚ÄòWeeklyStock.txt‚Äô saved [62683/62683]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "aC-IaBee4MfK",
        "colab_type": "text"
      },
      "source": [
        "### Load Data from local directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "button": false,
        "new_sheet": false,
        "run_control": {
          "read_only": false
        },
        "id": "mpM8dvNS4MfL",
        "colab_type": "code",
        "outputId": "e637f2be-99c1-46f3-9eb7-0ab6c8a55058",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"WeeklyStock.txt\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Year</th>\n",
              "      <th>Lag1</th>\n",
              "      <th>Lag2</th>\n",
              "      <th>Lag3</th>\n",
              "      <th>Lag4</th>\n",
              "      <th>Lag5</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Today</th>\n",
              "      <th>Direction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1990</td>\n",
              "      <td>0.816</td>\n",
              "      <td>1.572</td>\n",
              "      <td>-3.936</td>\n",
              "      <td>-0.229</td>\n",
              "      <td>-3.484</td>\n",
              "      <td>0.154976</td>\n",
              "      <td>-0.270</td>\n",
              "      <td>Down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1990</td>\n",
              "      <td>-0.270</td>\n",
              "      <td>0.816</td>\n",
              "      <td>1.572</td>\n",
              "      <td>-3.936</td>\n",
              "      <td>-0.229</td>\n",
              "      <td>0.148574</td>\n",
              "      <td>-2.576</td>\n",
              "      <td>Down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1990</td>\n",
              "      <td>-2.576</td>\n",
              "      <td>-0.270</td>\n",
              "      <td>0.816</td>\n",
              "      <td>1.572</td>\n",
              "      <td>-3.936</td>\n",
              "      <td>0.159837</td>\n",
              "      <td>3.514</td>\n",
              "      <td>Up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1990</td>\n",
              "      <td>3.514</td>\n",
              "      <td>-2.576</td>\n",
              "      <td>-0.270</td>\n",
              "      <td>0.816</td>\n",
              "      <td>1.572</td>\n",
              "      <td>0.161630</td>\n",
              "      <td>0.712</td>\n",
              "      <td>Up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1990</td>\n",
              "      <td>0.712</td>\n",
              "      <td>3.514</td>\n",
              "      <td>-2.576</td>\n",
              "      <td>-0.270</td>\n",
              "      <td>0.816</td>\n",
              "      <td>0.153728</td>\n",
              "      <td>1.178</td>\n",
              "      <td>Up</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Year   Lag1   Lag2   Lag3   Lag4   Lag5    Volume  Today Direction\n",
              "0  1990  0.816  1.572 -3.936 -0.229 -3.484  0.154976 -0.270      Down\n",
              "1  1990 -0.270  0.816  1.572 -3.936 -0.229  0.148574 -2.576      Down\n",
              "2  1990 -2.576 -0.270  0.816  1.572 -3.936  0.159837  3.514        Up\n",
              "3  1990  3.514 -2.576 -0.270  0.816  1.572  0.161630  0.712        Up\n",
              "4  1990  0.712  3.514 -2.576 -0.270  0.816  0.153728  1.178        Up"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHDtvsjI4MfO",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"preprocessing\">Data pre-processing and selection</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iaz1JmefHnyT",
        "colab_type": "code",
        "outputId": "51f4d648-842c-40f1-eef2-977bc792d2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "df.describe()\n",
        "df['Year'].unique()\n",
        "print(df.shape)\n",
        "df['Direction'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1089, 9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Up      605\n",
              "Down    484\n",
              "Name: Direction, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hoHyEprJNlm",
        "colab_type": "text"
      },
      "source": [
        "Since this is a time series data (weekly) from 1990 to 2010, we'll train our model using past years and validate the model using the most recent years.\n",
        "\n",
        "**Training Data**: 1990 - 2008\n",
        "\n",
        "**Validation Data**: 2009 - 2010\n",
        "\n",
        "Split the data accordingly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGNBN_SkHnup",
        "colab_type": "code",
        "outputId": "1ad7f7c5-3532-4cf7-9c31-9925b1aaa996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "recentYear = df['Year'] >= 2009\n",
        "val_df = df[recentYear]\n",
        "train_df = df[~recentYear]\n",
        "print('Training Data:', train_df.shape, '| Validation Data:', val_df.shape)\n",
        "\n",
        "#train_df.describe()\n",
        "#val_df.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data: (985, 9) | Validation Data: (104, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lMCRkdxs7im",
        "colab_type": "text"
      },
      "source": [
        "Train / test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGejzVeK4Mfd",
        "colab_type": "code",
        "outputId": "407dee84-6494-4de6-b22e-e43c6b83e605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train, y_train = (train_df.iloc[:, 1:-1], train_df['Direction'])\n",
        "X_train[:5]\n",
        "y_train[:5]\n",
        "\n",
        "X_val, y_val = (val_df.iloc[:, 1:-1], val_df['Direction'])\n",
        "#X_val.shape\n",
        "y_val.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(104,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEzbXpXC4Mft",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"modeling\">Modeling (Logistic Regression with Scikit-learn)</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwcQwoWO4Mfv",
        "colab_type": "text"
      },
      "source": [
        "Lets build our model using __LogisticRegression__ from Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including ‚Äònewton-cg‚Äô, ‚Äòlbfgs‚Äô, ‚Äòliblinear‚Äô, ‚Äòsag‚Äô, ‚Äòsaga‚Äô solvers. You can find extensive information about the pros and cons of these optimizers if you search it in internet.\n",
        "\n",
        "The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem in machine learning models.\n",
        "__C__ parameter indicates __inverse of regularization strength__ which must be a positive float. Smaller values specify stronger regularization. \n",
        "Now lets fit our model with train set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hTe-Y8D4Mfw",
        "colab_type": "code",
        "outputId": "a3b8fc4b-313a-405a-aae2-d623e546b620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "modelLogit = LogisticRegression(C=0.01, solver='liblinear').fit(X_train, y_train)\n",
        "modelLogit"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9dNT4JI4Mfz",
        "colab_type": "code",
        "outputId": "89619db5-9460-4603-e373-188e83795bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "yhat = modelLogit.predict(X_val)\n",
        "yhat[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Down', 'Down', 'Down', 'Down', 'Up', 'Down', 'Down', 'Down',\n",
              "       'Down', 'Up'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uTk0FRk4Mf1",
        "colab_type": "text"
      },
      "source": [
        "__predict_proba__  returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class **Down** i.e. P(Y=Down|X), and second column is probability of class **Up** i.e P(Y=Up|X):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj2falWn4Mf2",
        "colab_type": "code",
        "outputId": "e02e328d-40c3-4e1d-c11e-25450f9c29f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "prob = modelLogit.predict_proba(X_val)\n",
        "np.around(prob[:10], decimals=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.  , 0.  ],\n",
              "       [1.  , 0.  ],\n",
              "       [0.94, 0.07],\n",
              "       [0.7 , 0.3 ],\n",
              "       [0.  , 1.  ],\n",
              "       [1.  , 0.  ],\n",
              "       [1.  , 0.  ],\n",
              "       [1.  , 0.  ],\n",
              "       [1.  , 0.  ],\n",
              "       [0.  , 1.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LfqfSab4Mf4",
        "colab_type": "text"
      },
      "source": [
        "<h2 id=\"evaluation\">Evaluation</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spyobal24Mf4",
        "colab_type": "text"
      },
      "source": [
        "###1. jaccard index\n",
        "Lets try jaccard index for accuracy evaluation. we can define jaccard as the size of the intersection divided by the size of the union of two label sets. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b--g6-PV4Mf5",
        "colab_type": "code",
        "outputId": "0138713a-c916-4f68-b94f-1a2ae71fe6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "from sklearn.metrics import jaccard_similarity_score\n",
        "jaccard_similarity_score(y_val, yhat)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:635: DeprecationWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
            "  'and multiclass classification tasks.', DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9903846153846154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sEyoV6n4Mf8",
        "colab_type": "text"
      },
      "source": [
        "###2. confusion matrix\n",
        "Another way of looking at accuracy of classifier is to look at __confusion matrix__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGHoHJmz4Mf8",
        "colab_type": "code",
        "outputId": "02461dc7-645f-4859-e0f2-a11731ba2a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "\n",
        "def plot_cmx(cmx, classes, normalize=False, \n",
        "             title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cmx = cmx.astype('float') / cmx.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cmx)\n",
        "\n",
        "    plt.imshow(cmx, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cmx.max() / 2.\n",
        "    for i, j in itertools.product(range(cmx.shape[0]), range(cmx.shape[1])):\n",
        "        plt.text(j, i, format(cmx[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cmx[i, j] > thresh else \"black\")\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "print(confusion_matrix(y_val, yhat, labels=['Down', 'Up']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[42  1]\n",
            " [ 0 61]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDwHhIOB4MgC",
        "colab_type": "code",
        "outputId": "f3cbe076-e20e-419e-d855-0c572a77e6f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_val, yhat, labels=['Down','Up'])\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_cmx(cnf_matrix, classes=['Down','Up'],normalize=False,  title='Confusion matrix')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[42  1]\n",
            " [ 0 61]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEmCAYAAAAEH9kkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlGZmFThRmplV8P8BFye0IH3SdsUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZYvYCOV4MgH",
        "colab_type": "text"
      },
      "source": [
        "###3. Precison, Recall and F1-Score\n",
        "Based on the count of each section, we can calculate precision and recall of each label:\n",
        "\n",
        "\n",
        "- __Precision__ is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP¬†/¬†(TP¬†+¬†FP)\n",
        "\n",
        "- __Recall__ is true positive rate. It is defined as: Recall = ¬†TP¬†/¬†(TP¬†+¬†FN)\n",
        "\n",
        "    \n",
        "So, we can calculate precision and recall of each class.\n",
        "\n",
        "__F1 score:__\n",
        "Now we are in the position to calculate the F1 scores for each label based on the precision and recall of that label. \n",
        "\n",
        "The F1 score is the harmonic average of the¬†precision and recall, where an F1¬†score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP7ItfMM4MgF",
        "colab_type": "code",
        "outputId": "bdd0b00a-9c21-4513-b80f-7a0e93e95ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "print (classification_report(y_val, yhat))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Down       1.00      0.98      0.99        43\n",
            "          Up       0.98      1.00      0.99        61\n",
            "\n",
            "    accuracy                           0.99       104\n",
            "   macro avg       0.99      0.99      0.99       104\n",
            "weighted avg       0.99      0.99      0.99       104\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWPP-ZN54MgI",
        "colab_type": "text"
      },
      "source": [
        "###4. log loss\n",
        "Now, lets try __log loss__ for evaluation. This probability is a value between 0 and 1.\n",
        "Log loss(¬†Logarithmic¬†loss) measures the performance of a¬†classifier¬†where the predicted output is a probability value between 0 and 1. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxfd4Yl64MgJ",
        "colab_type": "code",
        "outputId": "048aab21-32cf-4ede-c726-c832fd3683ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "log_loss(y_val, prob)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18044781365124105"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW0QbNnYREPI",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion\n",
        "We achieve an accuracy of close to 100% and the confusion matrix shows a misclassification of only 1 observation. Achieving a nearly perfect model is good but it might also be an indication of overfitting.\n",
        "In this dataset, there is a **cheat variable** : this variable alone is can predict our target i.e. Direction. Exclude this variable and run the model again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3COU5XAE3lBb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h3>Thanks for reading!</h3>\n",
        "\n",
        "<h4>Author:  <a href=\"https://www.linkedin.com/in/ademolabuwo\">Ademola Arigbabuwo</a></h4>\n"
      ]
    }
  ]
}